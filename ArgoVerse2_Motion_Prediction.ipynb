{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependenceis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf\n",
    "!pip install  pyntcloud\n",
    "!pip install open3d\n",
    "!pip install OpenCV\n",
    "!pip install Plotly\n",
    "!pip install psutil requests\n",
    "!apt install aria2\n",
    "!apt-get install -y orca\n",
    "!pip install torch #should be already downloaded on google collab, including but limiting those above \n",
    "import tarfile\n",
    "import os\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import Button, VBox\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as pyplot\n",
    "import pyarrow.feather as feather\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for train and test files if they don't exist\n",
    "!mkdir -p /argotrain\n",
    "#!mkdir -p /argotest\n",
    "\n",
    "# Train 1\n",
    "!aria2c -x 16 https://s3.amazonaws.com/argoverse/datasets/av2/tars/motion-forecasting/train.tar -d /argotrain -o train-000.tar\n",
    "\n",
    "# Test 1\n",
    "#!aria2c -x 16 https://s3.amazonaws.com/argoverse/datasets/av2/tars/sensor/test-000.tar -d /argotest -o test-000.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deccompressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('/argotrain/train-000.tar', 'r') as tar:\n",
    "    tar.extractall('/argotrain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_parquet_files = os.listdir(\"/argotrain/train\")\n",
    "#\"/argotrain/train/0000b0f9-99f9-4a1f-a231-5be9e4c523f7/scenario_0000b0f9-99f9-4a1f-a231-5be9e4c523f7.parquet\"\n",
    "list_of_parquet_files_string=[]\n",
    "for i in range(199908):\n",
    "  list_of_parquet_files_string.append(\"/argotrain/train/\"+list_of_parquet_files[i]+\"/scenario_\"+list_of_parquet_files[i]+\".parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "covered_time = 80      # Observed timesteps\n",
    "future_time = 30       # Future timesteps\n",
    "max_objects = 80       # Maximum number of objects per scenario, made through analysis on average count of objects per scneriar, mean was 55.5\n",
    "bins = 120              # Number of bins for discretization, made arbitrarly, no reason behind it\n",
    "batch_size = 16        # Batch size for training\n",
    "num_epochs = 100        # Number of training epochs\n",
    "learning_rate = 1e-4   # Learning rate for optimizer\n",
    "\n",
    "# Pre-determined Global Bin Ranges (Set based on our prior analysis, check the ReadMe for more info at https://github.com/hashemJaber/Motion-Predction-for-cars)\n",
    "x_range_min = 0.0\n",
    "x_range_max = 41.7139\n",
    "y_range_min = -5.5628\n",
    "y_range_max = 6.2710\n",
    "heading_range_min = -0.7035\n",
    "heading_range_max = 0.7346\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract World Trajectories\n",
    "def extract_world_trajectories(parquet_path:str, max_objects:int=80, observed_timesteps:int=80):\n",
    "    \"\"\"\n",
    "    Extract world trajectories for the observed timesteps.\n",
    "    Args:\n",
    "        parquet_path: str  (Path to the .parquet file)\n",
    "        max_objects: int (Maximum number of objects to extract)\n",
    "        observed_timesteps: int (The number of timesteps to include as input (i.e., the history up to t))\n",
    "    Returns:\n",
    "        World trajectory array of shape (max_objects, observed_timesteps, feature_dim).\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = df.sort_values(by=['track_id', 'timestep'])\n",
    "    features = ['position_x', 'position_y', 'heading', 'velocity_x', 'velocity_y', 'object_category']\n",
    "\n",
    "    grouped = df.groupby('track_id')\n",
    "    objects = [object_group[features].to_numpy()[:observed_timesteps]\n",
    "               for _, object_group in grouped if len(object_group) >= observed_timesteps]\n",
    "\n",
    "\n",
    "    # Pad or truncate to max_objects, maybe a bad opition subject to change\n",
    "    num_objects = min(len(objects), max_objects)\n",
    "    objects_array = np.zeros((max_objects, observed_timesteps, len(features)))\n",
    "    objects_array[:num_objects] = objects[:num_objects]\n",
    "    #print(\"objects_array: \",objects_array.shape)\n",
    "    #print(\"extract_world_trajectories: \",objects_array)\n",
    "\n",
    "    return objects_array\n",
    "\n",
    "\n",
    "\n",
    "# 2. Extract Focused Object Trajectory\n",
    "def extract_focused_object_trajectory(parquet_path:str, observed_timesteps:int=80):\n",
    "    \"\"\"\n",
    "    Extract the focused object's trajectory up to the observed timesteps.\n",
    "\n",
    "    NOTE: THIS MIGHT SEEM REDUNTANT FOR NOW, DUE TO THE WORLD TRAJECTORIES ALREADY INCLUDING THIS INFO\n",
    "    BUT THIS IS SUBJECT TO CHANGE ONCE WE RESOLVE THE HD MAP REPRESENATION\n",
    "\n",
    "    Args:\n",
    "        parquet_path: str (Path to the .parquet file)\n",
    "        observed_timesteps: int (The number of timesteps to include as input (i.e., the history up to t))\n",
    "\n",
    "    Returns:\n",
    "        Focused object trajectory array of shape (observed_timesteps, feature_dim).\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    focal_track_id = df['focal_track_id'].iloc[0]\n",
    "    focused_object_df = df[df['track_id'] == focal_track_id]\n",
    "\n",
    "    features = ['position_x', 'position_y', 'heading', 'velocity_x', 'velocity_y', 'object_category']\n",
    "    focused_object_df=focused_object_df[features].to_numpy()[:observed_timesteps]\n",
    "    #print(\"focused_object_df: \",focused_object_df.shape)\n",
    "\n",
    "    # Pad or truncate to max_objects\n",
    "    #num_objects = min(len(objects), max_objects)\n",
    "    #print(\"focused_object_df: \",focused_object_df.shape)\n",
    "    #print(\"focused_object_df: \",focused_object_df[:1])\n",
    "    return focused_object_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/74/0x8rjbtd3xb99tjy9f_2dth80000gn/T/ipykernel_94752/124414281.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,  map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keys': ['model_state_dict', 'optimizer_state_dict', 'epoch', 'loss'],\n",
       " 'model_state_dict_keys': ['src_tok_emb.weight',\n",
       "  'src_tok_emb.bias',\n",
       "  'tgt_tok_emb.weight',\n",
       "  'tgt_tok_emb.bias',\n",
       "  'positional_encoding.pos_embedding',\n",
       "  'transformer.encoder.layers.0.self_attn.in_proj_weight',\n",
       "  'transformer.encoder.layers.0.self_attn.in_proj_bias',\n",
       "  'transformer.encoder.layers.0.self_attn.out_proj.weight',\n",
       "  'transformer.encoder.layers.0.self_attn.out_proj.bias',\n",
       "  'transformer.encoder.layers.0.linear1.weight',\n",
       "  'transformer.encoder.layers.0.linear1.bias',\n",
       "  'transformer.encoder.layers.0.linear2.weight',\n",
       "  'transformer.encoder.layers.0.linear2.bias',\n",
       "  'transformer.encoder.layers.0.norm1.weight',\n",
       "  'transformer.encoder.layers.0.norm1.bias',\n",
       "  'transformer.encoder.layers.0.norm2.weight',\n",
       "  'transformer.encoder.layers.0.norm2.bias',\n",
       "  'transformer.encoder.norm.weight',\n",
       "  'transformer.encoder.norm.bias',\n",
       "  'transformer.decoder.layers.0.self_attn.in_proj_weight',\n",
       "  'transformer.decoder.layers.0.self_attn.in_proj_bias',\n",
       "  'transformer.decoder.layers.0.self_attn.out_proj.weight',\n",
       "  'transformer.decoder.layers.0.self_attn.out_proj.bias',\n",
       "  'transformer.decoder.layers.0.multihead_attn.in_proj_weight',\n",
       "  'transformer.decoder.layers.0.multihead_attn.in_proj_bias',\n",
       "  'transformer.decoder.layers.0.multihead_attn.out_proj.weight',\n",
       "  'transformer.decoder.layers.0.multihead_attn.out_proj.bias',\n",
       "  'transformer.decoder.layers.0.linear1.weight',\n",
       "  'transformer.decoder.layers.0.linear1.bias',\n",
       "  'transformer.decoder.layers.0.linear2.weight',\n",
       "  'transformer.decoder.layers.0.linear2.bias',\n",
       "  'transformer.decoder.layers.0.norm1.weight',\n",
       "  'transformer.decoder.layers.0.norm1.bias',\n",
       "  'transformer.decoder.layers.0.norm2.weight',\n",
       "  'transformer.decoder.layers.0.norm2.bias',\n",
       "  'transformer.decoder.layers.0.norm3.weight',\n",
       "  'transformer.decoder.layers.0.norm3.bias',\n",
       "  'transformer.decoder.norm.weight',\n",
       "  'transformer.decoder.norm.bias',\n",
       "  'generator_x.weight',\n",
       "  'generator_x.bias',\n",
       "  'generator_y.weight',\n",
       "  'generator_y.bias',\n",
       "  'generator_heading.weight',\n",
       "  'generator_heading.bias'],\n",
       " 'optimizer_state_dict_keys': ['state', 'param_groups'],\n",
       " 'epoch': 0,\n",
       " 'loss': 6.784735815025145}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "checkpoint_path = '/Users/hashemjaber/Desktop/Motion-Predction-for-cars/best_model.pth' #replace this with your own path !\n",
    "checkpoint = torch.load(checkpoint_path,  map_location=torch.device('cpu'))\n",
    "\n",
    "checkpoint_contents = {\n",
    "    \"keys\": list(checkpoint.keys()),\n",
    "    \"model_state_dict_keys\": list(checkpoint['model_state_dict'].keys()) if 'model_state_dict' in checkpoint else \"Not Available\",\n",
    "    \"optimizer_state_dict_keys\": list(checkpoint['optimizer_state_dict'].keys()) if 'optimizer_state_dict' in checkpoint else \"Not Available\",\n",
    "    \"epoch\": checkpoint['epoch'] if 'epoch' in checkpoint else \"Not Available\",\n",
    "    \"loss\": checkpoint['loss'] if 'loss' in checkpoint else \"Not Available\",\n",
    "}\n",
    "\n",
    "checkpoint_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
